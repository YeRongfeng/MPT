"""
Optim.py - Transformer学习率调度优化器

【核心功能】
本模块实现了Transformer模型专用的学习率调度器，采用预热(warm-up)和衰减策略，
确保模型训练的稳定性和收敛性。这是Transformer训练成功的关键组件之一。

【技术特点】
1. 预热机制：训练初期逐步增加学习率，避免梯度爆炸
2. 衰减策略：预热后按特定公式衰减学习率，保证收敛
3. 自适应调整：根据模型维度和训练步数动态调整
4. 包装器设计：兼容PyTorch标准优化器接口
5. 数学驱动：基于Transformer论文的学习率公式

【学习率公式】
lr = lr_mul * d_model^(-0.5) * min(step^(-0.5), step * warmup_steps^(-1.5))

其中：
- 预热阶段(step < warmup_steps)：lr ∝ step * warmup_steps^(-1.5)
- 衰减阶段(step >= warmup_steps)：lr ∝ step^(-0.5)

【设计原理】
1. 预热阶段：线性增长，避免训练初期的不稳定
2. 衰减阶段：平方根衰减，保证长期收敛
3. 模型维度缩放：大模型需要更小的学习率
4. 步数自适应：根据训练进度自动调整

技术栈：
- NumPy 数值计算库
- PyTorch优化器接口兼容
- 数学公式驱动的调度策略

使用场景：
- Transformer模型训练
- 大规模深度学习模型优化
- 需要预热策略的训练任务
- 学习率敏感的模型训练

参考文献：
- Attention Is All You Need (Vaswani et al., 2017)
- https://github.com/jadore801120/attention-is-all-you-need-pytorch
"""

import numpy as np

class ScheduledOptim():
    """
    ScheduledOptim - 学习率调度优化器包装器
    
    【系统概述】
    这是一个学习率调度器的包装类，实现了Transformer论文中提出的学习率调度策略。
    它包装了标准的PyTorch优化器，在每个训练步骤中自动调整学习率。
    
    【核心思想】
    Transformer的学习率调度策略基于以下观察：
    1. 训练初期需要较小的学习率避免不稳定
    2. 预热后需要逐步衰减学习率保证收敛
    3. 模型维度越大，需要的学习率越小
    4. 学习率应该与训练进度相适应
    
    【调度策略】
    学习率调度分为两个阶段：
    1. 预热阶段(Warm-up)：学习率线性增长
       - 目的：避免训练初期的梯度爆炸
       - 公式：lr ∝ step_num
       - 持续：前n_warmup_steps步
    
    2. 衰减阶段(Decay)：学习率平方根衰减
       - 目的：保证模型长期收敛
       - 公式：lr ∝ step_num^(-0.5)
       - 持续：预热后的所有步骤
    
    【数学原理】
    完整的学习率公式：
    lr = lr_mul * d_model^(-0.5) * min(step^(-0.5), step * warmup_steps^(-1.5))
    
    分段解释：
    - 当step < warmup_steps时：lr = lr_mul * d_model^(-0.5) * step * warmup_steps^(-1.5)
    - 当step >= warmup_steps时：lr = lr_mul * d_model^(-0.5) * step^(-0.5)
    
    【设计优势】
    1. 训练稳定：预热机制避免初期不稳定
    2. 收敛保证：衰减策略确保长期收敛
    3. 自适应：根据模型大小和训练进度调整
    4. 兼容性：与PyTorch优化器完全兼容
    5. 简单易用：自动化的学习率管理
    
    【应用场景】
    1. Transformer模型训练：原始设计目标
    2. 大规模语言模型：GPT、BERT等模型训练
    3. 序列到序列任务：机器翻译、文本摘要等
    4. 注意力机制模型：所有基于注意力的模型
    
    在MPT框架中的定位：
    - 训练优化核心：确保模型训练的稳定性和效果
    - 学习率管理器：自动化的学习率调度
    - 训练稳定器：防止训练过程中的不稳定现象
    """

    def __init__(self, optimizer, lr_mul, d_model, n_warmup_steps):
        """
        初始化学习率调度优化器
        
        【核心功能】
        设置学习率调度的所有必要参数，包括基础优化器、缩放因子、
        模型维度和预热步数。
        
        【设计原理】
        1. 优化器包装：保持与PyTorch优化器的兼容性
        2. 参数存储：保存调度所需的所有超参数
        3. 状态初始化：设置训练步数计数器
        4. 灵活配置：支持不同的调度参数组合
        
        Args:
            optimizer (torch.optim.Optimizer): 被包装的PyTorch优化器
                类型：如Adam、SGD、AdamW等标准优化器
                作用：执行实际的参数更新
                要求：必须是PyTorch标准优化器实例
                示例：torch.optim.Adam(model.parameters(), betas=(0.9, 0.98), eps=1e-9)
            lr_mul (float): 学习率乘数因子
                作用：整体缩放学习率的大小
                范围：通常为0.1-2.0
                调优：根据具体任务和数据调整
                影响：直接影响训练速度和稳定性
                示例：0.5表示使用标准公式的一半学习率
            d_model (int): 模型的特征维度
                作用：用于学习率的维度缩放
                来源：Transformer模型的d_model参数
                原理：大模型需要更小的学习率
                计算：学习率会除以√d_model
                示例：512维模型的缩放因子为1/√512≈0.044
            n_warmup_steps (int): 预热步数
                作用：决定预热阶段的长度
                范围：通常为1000-10000步
                设置：约为总训练步数的1-10%
                影响：影响训练初期的稳定性
                推荐：4000步是常见的设置
        
        【参数设置指导】
        1. lr_mul的选择：
           - 从1.0开始，根据训练效果调整
           - 训练不稳定时减小，收敛太慢时增大
           - 通常在0.5-2.0之间
        
        2. n_warmup_steps的选择：
           - 小数据集：1000-4000步
           - 大数据集：4000-10000步
           - 复杂任务：可以设置更长的预热期
        
        3. d_model的影响：
           - 自动从模型配置获取
           - 不需要手动调整
           - 确保与实际模型维度一致
        
        【初始化过程】
        1. 保存优化器引用：用于实际的参数更新
        2. 存储调度参数：用于学习率计算
        3. 初始化步数计数器：跟踪训练进度
        4. 准备调度状态：为后续调度做准备
        """
        self._optimizer = optimizer  # 保存被包装的优化器：用于执行实际的参数更新操作
        self.lr_mul = lr_mul  # 保存学习率乘数：用于整体缩放学习率大小
        self.d_model = d_model  # 保存模型维度：用于学习率的维度缩放计算
        self.n_warmup_steps = n_warmup_steps  # 保存预热步数：决定预热阶段的长度
        self.n_steps = 0  # 初始化步数计数器：跟踪当前的训练步数


    def step_and_update_lr(self):
        """
        执行参数更新并同时更新学习率
        
        【核心功能】
        这是训练循环中调用的主要方法，它同时完成两个关键操作：
        1. 更新学习率：根据当前步数计算新的学习率
        2. 执行优化步骤：使用新学习率更新模型参数
        
        【执行顺序】
        1. 首先更新学习率：确保使用最新的学习率
        2. 然后执行优化步骤：用新学习率更新参数
        
        【使用方式】
        在训练循环中替代optimizer.step()：
        ```python
        # 传统方式
        optimizer.step()
        
        # 使用调度器
        scheduled_optimizer.step_and_update_lr()
        ```
        
        【设计考虑】
        1. 原子操作：学习率更新和参数更新作为一个整体
        2. 顺序保证：确保学习率在参数更新前更新
        3. 简化接口：用户只需调用一个方法
        4. 状态同步：保持步数计数器与实际训练同步
        
        【调用时机】
        - 在每个训练批次的反向传播后调用
        - 替代标准优化器的step()方法
        - 确保在zero_grad()之后调用
        
        【注意事项】
        - 每次调用都会增加步数计数器
        - 学习率会根据新的步数重新计算
        - 不要与optimizer.step()混用
        """
        self._update_learning_rate()  # 更新学习率：根据当前步数计算并设置新的学习率
        self._optimizer.step()  # 执行优化步骤：使用更新后的学习率进行参数更新


    def zero_grad(self):
        """
        清零梯度
        
        【核心功能】
        清除所有参数的梯度，为下一次反向传播做准备。这是标准优化器接口的一部分。
        
        【设计原理】
        1. 接口兼容：保持与PyTorch优化器的一致性
        2. 简单代理：直接调用内部优化器的方法
        3. 透明包装：用户感受不到包装器的存在
        
        【使用方式】
        在训练循环中的标准用法：
        ```python
        scheduled_optimizer.zero_grad()  # 清零梯度
        loss.backward()                  # 反向传播
        scheduled_optimizer.step_and_update_lr()  # 更新参数和学习率
        ```
        
        【调用时机】
        - 在每个训练批次开始时调用
        - 在loss.backward()之前调用
        - 确保梯度不会累积
        
        【重要性】
        - 防止梯度累积：PyTorch默认累积梯度
        - 保证训练正确性：每个批次使用独立的梯度
        - 内存管理：及时释放梯度占用的内存
        """
        self._optimizer.zero_grad()  # 清零梯度：调用内部优化器的zero_grad方法，清除所有参数的梯度


    def _get_lr_scale(self):
        """
        计算学习率缩放因子
        
        【核心功能】
        根据Transformer论文中的学习率公式计算当前步数对应的缩放因子。
        这是整个调度策略的数学核心。
        
        【数学公式】
        scale = d_model^(-0.5) * min(step^(-0.5), step * warmup_steps^(-1.5))
        
        【公式解析】
        1. d_model^(-0.5)：模型维度缩放
           - 大模型使用更小的学习率
           - 与模型复杂度成反比
           - 保证不同大小模型的训练稳定性
        
        2. min(step^(-0.5), step * warmup_steps^(-1.5))：分段函数
           - 预热阶段：step * warmup_steps^(-1.5)，线性增长
           - 衰减阶段：step^(-0.5)，平方根衰减
           - min函数实现平滑过渡
        
        【分阶段分析】
        1. 预热阶段(step < warmup_steps)：
           - 使用：step * warmup_steps^(-1.5)
           - 特点：学习率随步数线性增长
           - 目的：避免训练初期的不稳定
           - 效果：从0逐步增长到峰值
        
        2. 衰减阶段(step >= warmup_steps)：
           - 使用：step^(-0.5)
           - 特点：学习率随步数平方根衰减
           - 目的：保证模型长期收敛
           - 效果：从峰值逐步衰减
        
        Returns:
            float: 学习率缩放因子
                范围：(0, peak_scale]，其中peak_scale在warmup结束时达到
                特性：先增后减的单峰函数
                用途：与lr_mul相乘得到最终学习率
        
        【设计优势】
        1. 数学严谨：基于理论分析和实验验证
        2. 自适应：根据训练进度自动调整
        3. 稳定性：预热机制保证训练稳定
        4. 收敛性：衰减机制保证最终收敛
        
        【实现细节】
        - 使用numpy的数学函数确保数值稳定性
        - min函数实现分段函数的平滑过渡
        - 所有计算都是标量运算，效率很高
        """
        d_model = self.d_model  # 获取模型维度：用于维度缩放计算
        n_steps, n_warmup_steps = self.n_steps, self.n_warmup_steps  # 获取当前步数和预热步数
        return (d_model ** -0.5) * min(n_steps ** (-0.5), n_steps * n_warmup_steps ** (-1.5))  # 计算学习率缩放因子：实现Transformer论文中的学习率公式


    def _update_learning_rate(self):
        """
        更新学习率
        
        【核心功能】
        根据当前训练步数计算新的学习率，并更新优化器中所有参数组的学习率。
        这是学习率调度的核心实现。
        
        【更新流程】
        1. 增加步数计数器：跟踪训练进度
        2. 计算缩放因子：使用数学公式计算
        3. 计算最终学习率：缩放因子乘以基础乘数
        4. 更新所有参数组：设置新的学习率
        
        【学习率计算】
        lr = lr_mul * scale_factor
        其中scale_factor由_get_lr_scale()计算得出
        
        【参数组处理】
        PyTorch优化器支持不同的参数组使用不同的超参数，
        这里统一更新所有参数组的学习率，保证一致性。
        
        【步数管理】
        每次调用都会增加步数计数器，确保学习率随训练进度变化。
        步数从1开始计数，避免除零错误。
        
        【实现细节】
        1. 步数递增：self.n_steps += 1
           - 确保每次调用都更新步数
           - 从1开始避免数学计算错误
           - 与训练批次一一对应
        
        2. 学习率计算：lr = self.lr_mul * self._get_lr_scale()
           - 结合用户设置的乘数和数学公式
           - 实现完整的调度策略
           - 保证数值稳定性
        
        3. 参数组更新：遍历所有参数组
           - 支持复杂的优化器配置
           - 保证所有参数使用相同学习率
           - 兼容PyTorch的参数组机制
        
        【调用时机】
        - 在每次step_and_update_lr()中自动调用
        - 不应该直接调用此私有方法
        - 确保与参数更新同步
        
        【学习率变化模式】
        1. 第1步到第n_warmup_steps步：线性增长
        2. 第n_warmup_steps+1步开始：平方根衰减
        3. 整体趋势：先升后降的单峰曲线
        4. 峰值位置：在第n_warmup_steps步达到
        
        【数值稳定性】
        - 使用稳定的数学计算
        - 避免除零和溢出错误
        - 确保学习率始终为正数
        """
        self.n_steps += 1  # 增加步数计数器：跟踪训练进度，从1开始计数避免数学计算错误
        lr = self.lr_mul * self._get_lr_scale()  # 计算新的学习率：基础乘数乘以根据步数计算的缩放因子

        for param_group in self._optimizer.param_groups:  # 遍历优化器的所有参数组
            param_group['lr'] = lr  # 更新参数组的学习率：设置新计算的学习率值